{
  "metadata": {
    "documents": [
      "1.1. Linear Models \u2014 scikit-learn 1.7.1 documentation.pdf",
      "MIT_Lecture1.pdf",
      "ML TB1.pdf",
      "ML TB2.pdf",
      "ML_HandsOn_Chapter1.pdf",
      "ML_Stanford_Notes.pdf"
    ],
    "persona": "Data Science Intern",
    "job_to_be_done": "Summarize key ML algorithms from training material",
    "timestamp": "2025-07-25T20:02:56.330698"
  },
  "results": [
    {
      "document": "ML TB2.pdf",
      "page": 18,
      "section_title": "10  MACHINE LEARNING  Below we describe a procedure that fir...",
      "importance_rank": 1,
      "subsections": [
        {
          "page": 18,
          "text": "10  MACHINE LEARNING  Below we describe a procedure that first derives such training examples from  the indirect training experience available to the learner, then adjusts the weights  wi to best fit these training examples.  1.2.4.1 ESTIMATING TRAINING VALUES  Recall that according to our formulation of the learning problem, the only training  information available to our learner is whether the game was eventually won or  lost. On the other hand, we require training examples that assign specific scores  to specific board states. While it is easy to assign a value to board states that  correspond to the end of the game, it is less obvious how to assign training values  to the more numerous intermediate board states that occur before the game's end.  Of course the fact that the game was eventually won or lost does not necessarily  indicate that every board state along the game path was necessarily good or bad.  For example, even if the program loses the game, it may still be the case that  board states occurring early in the game should be rated very highly and that the  cause of the loss was a subsequent poor move.  Despite the ambiguity inherent in estimating training values for intermediate  board states, one simple approach has been found to be surprisingly successful.  This approach is to assign the training value of Krain(b) for any intermediate board  state b to be ?(~uccessor(b)),  where ? is the learner's current approximation to  V and where Successor(b) denotes the next board state following b for which it  is again the program's turn to move (i.e., the board state following the program's  move and the opponent's response). This rule for estimating training values can  be summarized as  ~ u l k   for estimating training values.  V,,,i. (b) c c(~uccessor(b))  While it may seem strange to use the current version of f to estimate training  values that will be used to refine this very same function, notice that we are using  estimates of the value of the Successor(b) to estimate the value of board state b. In-  tuitively, we can see this will make sense if ? tends to be more accurate for board  states closer to game's end. In fact, under certain conditions (discussed in Chap-  ter 13) the approach of iteratively estimating training values based on estimates of  successor state values can be proven to converge toward perfect estimates of Vtrain.  1.2.4.2 ADJUSTING THE WEIGHTS  All that remains is to specify the learning algorithm for choosing the weights wi  to^  best fit the set of training examples {(b, Vtrain(b))}.  As a first step we must define  what we mean by the bestfit to the training data. One common approach is to  define the best hypothesis, or set of weights, as that which minimizes the squarg  error E between the training values and the values predicted by the hypothesis V ."
        }
      ]
    },
    {
      "document": "ML TB2.pdf",
      "page": 19,
      "section_title": "Thus, we seek the weights, or equivalently the c ,  that min...",
      "importance_rank": 2,
      "subsections": [
        {
          "page": 19,
          "text": "Thus, we seek the weights, or equivalently the c ,  that minimize E for the observed  training examples. Chapter 6 discusses settings in which minimizing the sum of  squared errors is equivalent to finding the most probable hypothesis given the  observed training data.  Several algorithms are known for finding weights of a linear function that  minimize E defined in this way. In our case, we require an algorithm that will  incrementally refine the weights as new training examples become available and  that will be robust to errors in these estimated training values. One such algorithm  is called the least mean squares, or LMS training rule. For each observed training  example it adjusts the weights a small amount in the direction that reduces the  error on this training example. As discussed in Chapter 4, this algorithm can be  viewed as performing a stochastic gradient-descent search through the space of  possible hypotheses (weight values) to minimize the squared enor E. The LMS  algorithm is defined as follows:  LMS weight update rule.  For each training example (b, Kmin(b))  Use the current weights to calculate ?(b)  For each weight mi, update it as  Here q is a small constant (e.g., 0.1) that moderates the size of the weight update.  To get an intuitive understanding for why this weight update rule works, notice  that when the error (Vtrain(b) - c(b)) is zero, no weights are changed. When  (V,,ain(b) - e(b)) is positive (i.e., when f(b) is too low), then each weight is  increased in proportion to the value of its corresponding feature. This will raise  the value of ?(b), reducing the error. Notice that if the value of some feature  xi is zero, then its weight is not altered regardless of the error, so that the only  weights updated are those whose features actually occur on the training example  board. Surprisingly, in certain settings this simple weight-tuning method can be  proven to converge to the least squared error approximation to the &,in values  (as discussed in Chapter 4).  1.2.5 The Final Design  The final design of our checkers learning system can be naturally described by four  distinct program modules that represent the central components in many learning  systems. These four modules, summarized in Figure 1.1, are as follows:  0 The Performance System is the module that must solve the given per-  formance task, in this case playing checkers, by using the learned target  function(s). It takes an instance of a new problem (new game) as input and  produces a trace of its solution (game history) as output. In our case, the"
        }
      ]
    },
    {
      "document": "ML TB1.pdf",
      "page": 35,
      "section_title": "1.2 Examples ofMachine Learning Applications x: milage 9 Fig...",
      "importance_rank": 3,
      "subsections": [
        {
          "page": 35,
          "text": "1.2 Examples ofMachine Learning Applications x: milage 9 Figure 1.2 A training dataset of used cars and the function fitted. For simplic- ity, milage is taken as the only input attribute and a linear model is used. forth. Training data can be collected by monitoring and recording the actions of a human driver. One can envisage other applications of regression where one is trying to optimize a function. l Let us say we want to build a machine that roasts coffee. The machine has many inputs that affect the quality: various settings of temperatures, times, coffee bean type, and so forth. We make a number of experiments and for different settings of these inputs, we measure the quality of the coffee, for example, as consumer satisfaction. To find the optimal setting, we fit a regression model linking these inputs to coffee quality and choose new points to sample near the optimum of the current model to look for a better configuration. We sample these points, check quality, and add these to the data and fit a new model. This is generally called response surface design. 1. I would like to thank Michael Jordan for this example."
        }
      ]
    },
    {
      "document": "MIT_Lecture1.pdf",
      "page": 26,
      "section_title": "MIT 6.036 Fall 2018 25 tant factor in the success of an ML a...",
      "importance_rank": 4,
      "subsections": [
        {
          "page": 26,
          "text": "MIT 6.036 Fall 2018 25 tant factor in the success of an ML application is the way that the features are chosen to be encoded by the human who is framing the learning problem. 2.1 Discrete features Getting a good encoding of discrete features is particularly important. You want to create \u201copportunities\u201d for the ML system to \ufb01nd the underlying regularities. Although there are machine-learning methods that have special mechanisms for handling discrete inputs, all the methods we consider in this class will assume the input vectors x are in Rd. So, we have to \ufb01gure out some reasonable strategies for turning discrete values into (vectors of) real numbers. We\u2019ll start by listing some encoding strategies, and then work through some examples. Let\u2019s assume we have some feature in our raw data that can take on one of k discrete values. \u2022 Numeric Assign each of these values a number, say 1.0/k, 2.0/k, . . . , 1.0. We might want to then do some further processing, as described in section 8.3. This is a sensible strategy only when the discrete values really do signify some sort of numeric quantity, so that these numerical values are meaningful. \u2022 Thermometer code If your discrete values have a natural ordering, from 1, . . . , k, but not a natural mapping into real numbers, a good strategy is to use a vector of length k binary variables, where we convert discrete input value 0 < j \u2a7dk into a vector in which the \ufb01rst j values are 1.0 and the rest are 0.0. This does not necessarily imply anything about the spacing or numerical quantities of the inputs, but does convey something about ordering. \u2022 Factored code If your discrete values can sensibly be decomposed into two parts (say the \u201cmake\u201d and \u201cmodel\u201d of a car), then it\u2019s best to treat those as two separate features, and choose an appropriate encoding of each one from this list. \u2022 One-hot code If there is no obvious numeric, ordering, or factorial structure, then the best strategy is to use a vector of length k, where we convert discrete input value 0 < j \u2a7dk into a vector in which all values are 0.0, except for the jth, which is 1.0. \u2022 Binary code It might be tempting for the computer scientists among us to use some binary code, which would let us represent k values using a vector of length log k. This is a bad idea! Decoding a binary code takes a lot of work, and by encoding your inputs this way, you\u2019d be forcing your system to learn the decoding algorithm. As an example, imagine that we want to encode blood types, which are drawn from the set {A+, A\u2212, B+, B\u2212, AB+, AB\u2212, O+, O\u2212}. There is no obvious linear numeric scaling or even ordering to this set. But there is a reasonable factoring, into two features: {A, B, AB, O} and {+, \u22121}. And, in fact, we can reasonably factor the \ufb01rst group into {A, notA}, {B, notB} So, here are two plausible encodings of the whole set: It is sensible (according to Wikipedia!) to treat O as having neither fea- ture A nor feature B. It is sensible (according to Wikipedia!) to treat O as having neither fea- ture A nor feature B. \u2022 Use a 6-D vector, with two dimensions to encode each of the factors using a one-hot encoding. \u2022 Use a 3-D vector, with one dimension for each factor, encoding its presence as 1.0 and absence as \u22121.0 (this is sometimes better than 0.0). In this case, AB+ would be (1.0, 1.0, 1.0) and O\u2212would be (\u22121.0, \u22121.0, \u22121.0). Study Question: How would you encode A+ in both of these approaches? Last Updated: 02/01/19 01:38:02"
        }
      ]
    },
    {
      "document": "ML TB1.pdf",
      "page": 361,
      "section_title": "342 14.7.1 CONTINGENCY TABLE (14.15) McNEMAR'S TEST 14.7.2 1...",
      "importance_rank": 5,
      "subsections": [
        {
          "page": 361,
          "text": "342 14.7.1 CONTINGENCY TABLE (14.15) McNEMAR'S TEST 14.7.2 14 Assessing and Comparing Classification Algorithms McNemar's Test Given a training set and a validation set, we use two algorithms to train two classifiers on the training set and test them on the validation set and compute their errors. A contingency table, like the one shown here, is an array of natural numbers in matrix form representing counts, or frequencies: eOO: Number of examples eOl: Number of examples misclassified by both misclassified by 1 but not 2 elO: Number of examples ell: Number of examples misclassified by 2 but not 1 correctly classified by both Under the null hypothesis that the classification algorithms have the same error rate, we expect eOI = elO and these to be equal to (eOI + elO) /2. We have the chi-square statistic with one degree of freedom (l eOI- elOl-l)2 X 2 - I eOI + elO and McNemar's test accepts the hypothesis that the two classification algorithms have the same error rate at significance level ()( if this value is less than or equal to X~.I\u00b7 X5.o5.1 = 3.84. K-Fold Cross-Validated Paired t Test This set uses K-fold cross-validation to get K training/validation set pairs. We use the two classification algorithms to train on the training sets 'Ii, i = 1, ... ,K, and test on the validation sets \\Ii. The error percent- ages of the classifiers on the validation sets are recorded as pl and pT. If the two classification algorithms have the same error rate, then we expect them to have the same mean, or equivalently, that the difference of their means is O. The difference in error rates on fold i is Pi = pI - PT. When this is done K times, we have a distribution of Pi containing K points. Given that pl and PT are both (approximately) normal, their difference Pi is also normal. The null hypothesis is that this distribution has 0 mean: Ho J1 = 0 HI J1 -1= 0 We define If=l Pi m= K"
        }
      ]
    },
    {
      "document": "ML TB2.pdf",
      "page": 343,
      "section_title": "CHAPTER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 335  ...",
      "importance_rank": 6,
      "subsections": [
        {
          "page": 343,
          "text": "CHAPTER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 335  Purely analytical learning methods offer the advantage of generalizing more  accurately from less data by using prior knowledge to guide learning. However,  they can be misled when given incorrect or insufficient prior knowledge. Purely  inductive methods offer the advantage that they require no explicit prior knowl-  edge and learn regularities based solely on the training data. However, they can  fail when given insufficient training data, and can be misled by the implicit in-  ductive bias they must adopt in order to generalize beyond the observed data.  Table 12.1 summarizes these complementary advantages and pitfalls of induc-  tive and analytical learning methods. This chapter considers the question of how  to combine the two into a single algorithm that captures the best aspects of  both.  The difference between inductive and analytical learning methods can be  seen in the nature of the justiJications that can be given for their learned hypothe-  ses. Hypotheses output by purely analytical learning methods such as PROLOG-  EBG carry a logical justification; the output hypothesis follows deductively from  the domain theory and training examples. Hypotheses output by purely inductive  learning methods such as BACKPROPAGATION  carry a statistical justification; the  output hypothesis follows from statistical arguments that the training sample is  sufficiently large that it is probably representative of the underlying distribution  of examples. This statistical justification for induction is clearly articulated in the  PAC-learning results discussed in Chapter 7.  Given that analytical methods provide logically justified hypotheses and in-  ductive methods provide statistically justified hypotheses, it is easy to see why  combining them would be useful: Logical justifications are only as compelling as  the assumptions, or prior knowledge, on which they are built. They are suspect or  powerless if prior knowledge is incorrect or unavailable. Statistical justifications  are only as compelling as the data and statistical assumptions on which they rest.  They are suspect or powerless when assumptions about the underlying distribu-  tions cannot be trusted or when data is scarce. In short, the two approaches work  well for different types of problems. By combining them we can hope to devise  a more general learning approach that covers a more broad range of learning  tasks.  Figure 12.1 summarizes a spectrum of learning problems that varies by the  availability of prior knowledge and training data. At one extreme, a large volume  Inductive learning  Analytical learning  Goal:  Hypothesis fits data  Hypothesis fits domain theory  Justification:  Statistical inference  Deductive inference  Advantagex  Requires little prior knowledge  Learns from scarce data  Pitfalls:  Scarce data, incorrect bias  Imperfect domain theory  TABLE 12.1  Comparison of purely analytical and purely inductive learning."
        }
      ]
    },
    {
      "document": "ML TB1.pdf",
      "page": 39,
      "section_title": "1.3 Notes 13 Almost all of science is fitting models to data...",
      "importance_rank": 7,
      "subsections": [
        {
          "page": 39,
          "text": "1.3 Notes 13 Almost all of science is fitting models to data. Scientists design exper- iments and make observations and collect data. They then try to extract knowledge by finding out simple models that explain the data they ob- served. This is called induction and is the process of extracting general rules from a set of particular cases. We are now at a point that such analysis of data can no longer be done by people, both because the amount of data is huge and because people who can do such analysis are rare and manual analysis is costly. There is thus a growing interest in computer models that can analyze data and extract information automatically from them, that is, learn. The methods we are going to discuss in the coming chapters have their origins in different scientific domains. Sometimes the same algorithm was independently invented in more than one field, following a different historical path. In statistics, going from particular observations to general descriptions is called inference and learning is called estimation. Classification is called discriminant analysis in statistics (McLachlan 1992; Hastie, Tib- shirani, and Friedman 2001). Before computers were cheap and abun- dant, statisticians could only work with small samples. Statisticians, be- ing mathematicians, worked mostly with simple parametric models that could be analyzed mathematically. In engineering, classification is called pattern recognition and the approach is nonparametric and much more empirical (Duda, Hart, and Stork 2001; Webb 1999). Machine learning is related to artificial intelligence (Russell and Norvig 1995) because an in- telligent system should be able to adapt to changes in its environment. Application areas like vision, speech, and robotics are also tasks that are best learned from sample data. In electrical engineering, research in signal processing resulted in adaptive computer vision and speech pro- grams. Among these, the development of hidden Markov models (HMM) for speech recognition is especially important. In the late 1980s with advances in VLSI technology and the possibil- ity of building parallel hardware containing thousands of processors, the field of artificial neural networks was reinvented as a possible the- ory to distribute computation over a large number of processing units (Bishop, 1995). Over time, it has been realized in the neural network community that most neural network learning algorithms have their ba- sis in statistics-for example, the multilayer perceptron is another class of nonparametric estimator-and claims of brain-like computation have started to fade."
        }
      ]
    },
    {
      "document": "ML TB2.pdf",
      "page": 116,
      "section_title": "greater detailt. The network in Figure 4.7 was trained using...",
      "importance_rank": 8,
      "subsections": [
        {
          "page": 116,
          "text": "greater detailt. The network in Figure 4.7 was trained using the algorithm shown  in Table 4.2, with initial weights set to random values in the interval (-0.1,0.1),  learning rate q = 0.3, and no weight momentum (i.e., a! = 0). Similar results  were obtained by using other learning rates and by including nonzero momentum.  The hidden unit encoding shown in Figure 4.7 was obtained after 5000 training  iterations through the outer loop of the algorithm (i.e., 5000 iterations through each  of the eight training examples). Most of the interesting weight changes occurred,  however, during the first 2500 iterations.  We can directly observe the effect of BACKPROPAGATION'S  gradient descent  search by plotting the squared output error as a function of the number of gradient  descent search steps. This is shown in the top plot of Figure 4.8. Each line in  this plot shows the squared output error summed over all training examples, for  one of the eight network outputs. The horizontal axis indicates the number of  iterations through the outermost loop of the BACKPROPAGATION  algorithm. As this  plot indicates, the sum of squared errors for each output decreases as the gradient  descent procedure proceeds, more quickly for some output units and less quickly  for others.  The evolution of the hidden layer representation can be seen in the second  plot of Figure 4.8. This plot shows the three hidden unit values computed by the  learned network for one of the possible inputs (in particular, 01000000). Again, the  horizontal axis indicates the number of training iterations. As this plot indicates,  the network passes through a number of different encodings before converging to  the final encoding given in Figure 4.7.  Finally, the evolution of individual weights within the network is illustrated  in the third plot of Figure 4.8. This plot displays the evolution of weights con-  necting the eight input units (and the constant 1 bias input) to one of the three  hidden units. Notice that significant changes in the weight values for this hidden  unit coincide with significant changes in the hidden layer encoding and output  squared errors. The weight that converges to a value near zero in this case is the  bias weight wo.  4.6.5 Generalization, Overfitting, and Stopping Criterion  In the description of t'le BACKPROPAGATION  algorithm in Table 4.2, the termination  condition for the algcrithm has been left unspecified. What is an appropriate con-  dition for terrninatinp the weight update loop? One obvious choice is to continue  training until the errcr E on the training examples falls below some predetermined  threshold. In fact, this is a poor strategy because BACKPROPAGATION  is suscepti-  ble to overfitting the training examples at the cost of decreasing generalization  accuracy over other unseen examples.  To see the dangers of minimizing the error over the training data, consider  how the error E varies with the number of weight iterations. Figure 4.9 shows  t ~ h e   source code to reproduce this example is available at http://www.cs.cmu.edu/-tom/mlbook.hhnl."
        }
      ]
    },
    {
      "document": "ML TB2.pdf",
      "page": 23,
      "section_title": "the available training examples. The LMS algorithm for fitti...",
      "importance_rank": 9,
      "subsections": [
        {
          "page": 23,
          "text": "the available training examples. The LMS algorithm for fitting weights achieves  this goal by iteratively tuning the weights, adding a correction to each weight  each time the hypothesized evaluation function predicts a value that differs from  the training value. This algorithm works well when the hypothesis representation  considered by the learner defines a continuously parameterized space of potential  hypotheses.  Many of the chapters in this book present algorithms that search a hypothesis  space defined by some underlying representation (e.g., linear functions, logical  descriptions, decision trees, artificial neural networks). These different hypothesis  representations are appropriate for learning different kinds of target functions. For  each of these hypothesis representations, the corresponding learning algorithm  takes advantage of a different underlying structure to organize the search through  the hypothesis space.  Throughout this book we will return to this perspective of learning as a  search problem in order to characterize learning methods by their search strategies  and by the underlying structure of the search spaces they explore. We will also  find this viewpoint useful in formally analyzing the relationship between the size  of the hypothesis space to be searched, the number of training examples available,  and the confidence we can have that a hypothesis consistent with the training data  will correctly generalize to unseen examples.  1.3.1 Issues in Machine Learning  Our checkers example raises a number of generic questions about machine learn-  ing. The field of machine learning, and much of this book, is concerned with  answering questions such as the following:  What algorithms exist for learning general target functions from specific  training examples? In what settings will particular algorithms converge to the  desired function, given sufficient training data? Which algorithms perform  best for which types of problems and representations?  How much training data is sufficient? What general bounds can be found  to relate the confidence in learned hypotheses to the amount of training  experience and the character of the learner's hypothesis space?  When and how can prior knowledge held by the learner guide the process  of generalizing from examples? Can prior knowledge be helpful even when  it is only approximately correct?  What is the best strategy for choosing a useful next training experience, and  how does the choice of this strategy alter the complexity of the learning  problem?  What is the best way to reduce the learning task to one or more function  approximation problems? Put another way, what specific functions should  the system attempt to learn? Can this process itself be automated?  How can the learner automatically alter its representation to improve its  ability to represent and learn the target function?"
        }
      ]
    },
    {
      "document": "ML TB2.pdf",
      "page": 186,
      "section_title": "be estimated from the training data is just the number of di...",
      "importance_rank": 10,
      "subsections": [
        {
          "page": 186,
          "text": "be estimated from the training data is just the number of distinct attribute values  times the number of distinct target values-a  much smaller number than if we  were to estimate the P(a1, a2 . . . an lvj) terms as first contemplated.  To summarize, the naive Bayes learning method involves a learning step in  which the various P(vj) and P(ai Jvj) terms are estimated, based on their frequen-  cies over the training data. The set of these estimates corresponds to the learned  hypothesis. This hypothesis is then used to classify each new instance by applying  the rule in Equation (6.20). Whenever the naive Bayes assumption of conditional  independence is satisfied, this naive Bayes classification VNB is identical to the  MAP classification.  One interesting difference between the naive Bayes learning method and  other learning methods we have considered is that there is no explicit search  through the space of possible hypotheses (in this case, the space of possible  hypotheses is the space of possible values that can be assigned to the various P(vj)  and P(ailvj) terms). Instead, the hypothesis is formed without searching, simply by  counting the frequency of various data combinations within the training examples.  6.9.1 An Illustrative Example  Let us apply the naive Bayes classifier to a concept learning problem we consid-  ered during our discussion of decision tree learning: classifying days according  to whether someone will play tennis. Table 3.2 from Chapter 3 provides a set  of 14 training examples of the target concept PlayTennis, where each day is  described by the attributes Outlook, Temperature, Humidity, and Wind. Here we  use the naive Bayes classifier and the training data from this table to classify the  following novel instance:  (Outlook = sunny, Temperature = cool, Humidity = high, Wind = strong)  Our task is to predict the target value (yes or no) of the target concept  PlayTennis for this new instance. Instantiating Equation (6.20) to fit the current  task, the target value VNB is given by  = argrnax P(vj)  P(0utlook = sunny)v,)P(Temperature = coolIvj)  vj~(yes,no]  Notice in the final expression that ai has been instantiated using the particular  attribute values of the new instance. To calculate VNB we now require 10 proba-  bilities that can be estimated from the training data. First, the probabilities of the  different target values can easily be estimated based on their frequencies over the  14 training examples  P(P1ayTennis = yes) = 9/14 = .64  P(P1ayTennis = no) = 5/14 = .36"
        }
      ]
    }
  ]
}